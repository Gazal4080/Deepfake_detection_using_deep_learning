{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3723297,
          "sourceType": "datasetVersion",
          "datasetId": 2222120
        },
        {
          "sourceId": 7639866,
          "sourceType": "datasetVersion",
          "datasetId": 841565
        },
        {
          "sourceId": 9177835,
          "sourceType": "datasetVersion",
          "datasetId": 5546853
        },
        {
          "sourceId": 9941146,
          "sourceType": "datasetVersion",
          "datasetId": 6112205
        },
        {
          "sourceId": 9947105,
          "sourceType": "datasetVersion",
          "datasetId": 6116647
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "NLP Project",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gazal4080/Deepfake_detection_using_deep_learning/blob/master/AIMedicalAssistance_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "jpmiller_layoutlm_path = kagglehub.dataset_download('jpmiller/layoutlm')\n",
        "yousefsaeedian_ai_medical_chatbot_path = kagglehub.dataset_download('yousefsaeedian/ai-medical-chatbot')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "0lipy4aaJ9ja"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install important library"
      ],
      "metadata": {
        "id": "R1kk_mSGJ9jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score sacrebleu evaluate torchsummary"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-11T12:49:04.611599Z",
          "iopub.execute_input": "2024-12-11T12:49:04.612242Z",
          "iopub.status.idle": "2024-12-11T12:49:17.522919Z",
          "shell.execute_reply.started": "2024-12-11T12:49:04.612195Z",
          "shell.execute_reply": "2024-12-11T12:49:17.521881Z"
        },
        "id": "ofxJAuqEJ9jf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "7zLDcvYRJ9jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import evaluate\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments\n",
        "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, TrainerCallback, T5Config\n",
        "\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import TensorDataset\n",
        "from torchsummary import summary\n",
        "\n",
        "from collections import defaultdict\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-12-11T08:08:47.409931Z",
          "iopub.execute_input": "2024-12-11T08:08:47.410269Z",
          "iopub.status.idle": "2024-12-11T08:08:47.420452Z",
          "shell.execute_reply.started": "2024-12-11T08:08:47.410231Z",
          "shell.execute_reply": "2024-12-11T08:08:47.419561Z"
        },
        "trusted": true,
        "id": "gOVZyltjJ9jg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning Data"
      ],
      "metadata": {
        "id": "Vrxh-FKQJ9jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Data\n",
        "df = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')\n",
        "\n",
        "# Sampel Data\n",
        "print(\"Data Sample\")\n",
        "print(df.head())\n",
        "\n",
        "#Null value\n",
        "print(\"Null Value Data\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# List of question words\n",
        "question_words = ['what', 'who', 'why', 'when', 'where', 'how', 'is', 'are', 'does', 'do', 'can', 'will', 'shall']\n",
        "\n",
        "# Ensure questions are lowercase for consistent filtering\n",
        "df['question'] = df['question'].str.lower()\n",
        "\n",
        "# Filter rows where the question starts with a question word\n",
        "df = df[df['question'].str.split().str[0].isin(question_words)]\n",
        "\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicates = df.duplicated()\n",
        "print(f\"Number of duplicate rows: {duplicates.sum()}\")\n",
        "\n",
        "# Remove duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Reset the index after removing duplicates\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Delete Unused column\n",
        "df = df.drop(columns=['source', 'focus_area'])\n",
        "\n",
        "#Table Info\n",
        "print(\"Table Info\")\n",
        "print(df.info())\n",
        "\n",
        "# Apply the function\n",
        "df = df.drop_duplicates(subset='question', keep='first').reset_index(drop=True)\n",
        "df = df.drop_duplicates(subset='answer', keep='first').reset_index(drop=True)\n",
        "\n",
        "#Drop rows with null values\n",
        "df = df.drop_duplicates(subset=['question', 'answer']).reset_index(drop=True)\n",
        "df['question'] = df['question'].fillna('').astype(str)\n",
        "df['answer'] = df['answer'].fillna('').astype(str)\n",
        "\n",
        "# Removing \"(are)\" in the dataset\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"\\(.*?\\)\", \"\", text)\n",
        "    text = re.sub(r'\\s+', ' ', text.strip().lower())\n",
        "    return text\n",
        "\n",
        "df['question'] = df['question'].apply(clean_text)\n",
        "df['answer'] = df['answer'].apply(clean_text)\n",
        "\n",
        "df['question'] = df['question'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
        "df['answer'] = df['answer'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
        "\n",
        "#Checking again of null values\n",
        "print(\"Null Value Data\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "#Check for Unique Data\n",
        "print(f\"Unique questions: {df['question'].nunique()}\")\n",
        "print(f\"Unique answers: {df['answer'].nunique()}\")\n",
        "\n",
        "#Checking again of the data info\n",
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-11T08:08:54.305815Z",
          "iopub.execute_input": "2024-12-11T08:08:54.306268Z",
          "iopub.status.idle": "2024-12-11T08:08:56.80342Z",
          "shell.execute_reply.started": "2024-12-11T08:08:54.306224Z",
          "shell.execute_reply": "2024-12-11T08:08:56.802463Z"
        },
        "trusted": true,
        "id": "KASXnG6oJ9jg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecting Model"
      ],
      "metadata": {
        "id": "X97qFwEXJ9jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5-small model and tokenizer\n",
        "model_name = \"t5-base\"\n",
        "config = T5Config.from_pretrained(model_name)\n",
        "config.dropout_rate = 0.1\n",
        "config.feed_forward_proj = \"gelu\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    config=config\n",
        ")\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tie weights explicitly\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Print model architecture summary\n",
        "# Print detailed model summary\n",
        "print(\"\\nDetailed Model Summary:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def summarize_model_by_type(model):\n",
        "    layer_summary = defaultdict(int)\n",
        "    param_summary = defaultdict(int)\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        layer_type = type(module).__name__\n",
        "        layer_summary[layer_type] += 1\n",
        "        param_summary[layer_type] += sum(p.numel() for p in module.parameters())\n",
        "\n",
        "    print(f\"{'Layer Type':<30}{'Count':<10}{'Parameters':<15}\")\n",
        "    print(\"=\" * 55)\n",
        "    for layer_type, count in layer_summary.items():\n",
        "        print(f\"{layer_type:<30}{count:<10}{param_summary[layer_type]:<15,}\")\n",
        "\n",
        "summarize_model_by_type(model)\n",
        "\n",
        "# Preprocess function for seq2seq task\n",
        "def preprocess_function(batch):\n",
        "    inputs = [f\"answer the following question: {q}\" for q in batch['question']]\n",
        "    targets = [f\"{a}\" for a in batch['answer']]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets,\n",
        "            max_length=64,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "    labels[\"input_ids\"][labels[\"input_ids\"] == tokenizer.pad_token_id] = -100\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Train-test split\n",
        "train_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "# print('Halo: ',val_dataset.column_names)\n",
        "\n",
        "# Preprocess datasets\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    num_proc=4,\n",
        ")\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    num_proc=4,\n",
        ")\n",
        "\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    # eval_steps=1000,\n",
        "    # save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=5e-4,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    lr_scheduler_type=\"cosine_with_restarts\",\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.05,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    # load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"exact_match\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",\n",
        "    gradient_accumulation_steps=2,\n",
        "    max_grad_norm=0.5,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    generation_max_length=64,\n",
        "    generation_num_beams=6,\n",
        "    dataloader_num_workers=4,\n",
        "    group_by_length=True,\n",
        "    remove_unused_columns=True,\n",
        "    label_smoothing_factor= 0.1\n",
        ")\n",
        "\n",
        "# training_args.label_smoothing_factor = 0.1\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding='longest',\n",
        ")\n",
        "\n",
        "# Create function to show exact match, BLEU and ROUGE\n",
        "def compute_metrics(eval_pred, tokenizer):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Decode predictions\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Normalize text\n",
        "    decoded_preds = [text.strip().lower() for text in decoded_preds]\n",
        "    decoded_labels = [text.strip().lower() for text in decoded_labels]\n",
        "\n",
        "    # Multiple metrics\n",
        "    exact_match = np.mean([p == l for p, l in zip(decoded_preds, decoded_labels)])\n",
        "\n",
        "    bleu_metric = evaluate.load(\"bleu\")\n",
        "    rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "    bleu_score = bleu_metric.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=[[label] for label in decoded_labels]\n",
        "    )[\"bleu\"]\n",
        "\n",
        "    rouge_score = rouge_metric.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels\n",
        "    )[\"rougeL\"]\n",
        "\n",
        "    return {\n",
        "        \"exact_match\": exact_match,\n",
        "        \"BLEU\": bleu_score,\n",
        "        \"ROUGE-L\": rouge_score,\n",
        "    }\n",
        "\n",
        "# Initialize data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding='longest',\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "trainer.save_model(\"./t5_chatbot_model\")\n",
        "tokenizer.save_pretrained(\"./t5_chatbot_tokenizer\")\n",
        "model_path = \"./t5_chatbot_model.h5\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Save log history\n",
        "log_history = trainer.state.log_history"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-12-11T08:09:01.331894Z",
          "iopub.execute_input": "2024-12-11T08:09:01.332257Z",
          "iopub.status.idle": "2024-12-11T09:22:30.2375Z",
          "shell.execute_reply.started": "2024-12-11T08:09:01.332223Z",
          "shell.execute_reply": "2024-12-11T09:22:30.236561Z"
        },
        "trusted": true,
        "id": "KsTVrT9yJ9jh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show graphics"
      ],
      "metadata": {
        "id": "q7umR396J9ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract loss values from log history\n",
        "train_loss = []\n",
        "eval_loss = []\n",
        "eval_bleu = []\n",
        "eval_exact_match = []\n",
        "eval_rogue = []\n",
        "steps = []\n",
        "eval_steps = []\n",
        "\n",
        "for log in log_history:\n",
        "    if \"loss\" in log:\n",
        "        train_loss.append(log[\"loss\"])\n",
        "        steps.append(log[\"step\"])\n",
        "    if \"eval_loss\" in log:\n",
        "        eval_loss.append(log[\"eval_loss\"])\n",
        "        eval_steps.append(log[\"step\"])\n",
        "    if \"eval_BLEU\" in log:\n",
        "        eval_bleu.append(log[\"eval_BLEU\"])\n",
        "    if \"eval_ROUGE-L\" in log:\n",
        "        eval_rogue.append(log[\"eval_ROUGE-L\"])\n",
        "    if \"eval_exact_match\" in log:\n",
        "        eval_exact_match.append(log[\"eval_exact_match\"])\n",
        "\n",
        "# Plot the losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(steps, train_loss, label=\"Training Loss\", color=\"blue\", marker=\"o\")\n",
        "plt.plot(steps[:len(eval_loss)], eval_loss, label=\"Evaluation Loss\", color=\"orange\", marker=\"o\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Evaluation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot the BLEU\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(eval_steps, eval_bleu, label=\"BLEU\", marker=\"o\", linestyle=\"-\", color=\"green\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Metric Score\")\n",
        "plt.title(\"BLEU Score Over Training Steps\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot the ROGUE\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(eval_steps, eval_bleu, label=\"ROGUE-L\", marker=\"o\", linestyle=\"-\", color=\"red\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Metric Score\")\n",
        "plt.title(\"ROGUE-L Score Over Training Steps\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot the BLEU\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(eval_steps, eval_bleu, label=\"Exact Match\", marker=\"o\", linestyle=\"-\", color=\"black\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Metric Score\")\n",
        "plt.title(\"Exact match Over Training Steps\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-11T09:22:30.23958Z",
          "iopub.execute_input": "2024-12-11T09:22:30.239916Z",
          "iopub.status.idle": "2024-12-11T09:22:31.370684Z",
          "shell.execute_reply.started": "2024-12-11T09:22:30.239881Z",
          "shell.execute_reply": "2024-12-11T09:22:31.369813Z"
        },
        "id": "5JP3ciTUJ9jj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing using Input"
      ],
      "metadata": {
        "id": "Os2GPtOaJ9jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained T5 model and tokenizer\n",
        "model_path = \"/kaggle/working/t5_chatbot_model\"\n",
        "tokenizer_path = \"/kaggle/working/t5_chatbot_tokenizer\"\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "# Generate responses using Top-K and Top-P sampling\n",
        "def generate_response_top_k_top_p(\n",
        "    question, model, tokenizer, max_length=64, top_k=50, top_p=0.95, temperature=1.0\n",
        "):\n",
        "    # Format the question for the model\n",
        "    formatted_question = f\"Answer the following question: {question}\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(\n",
        "        formatted_question,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "    )\n",
        "\n",
        "    # Generate response with top-k and top-p sampling\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=max_length,\n",
        "        do_sample=True,  # Enables sampling instead of greedy/beam search\n",
        "        top_k=top_k,  # Top-K sampling\n",
        "        top_p=top_p,  # Nucleus sampling\n",
        "        temperature=temperature,  # Adjusts randomness\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode the generated response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "question = \"What is alzheimer?\"\n",
        "response = generate_response_top_k_top_p(question, model, tokenizer)\n",
        "print(\"Question:\", question)\n",
        "print(\"Response:\", response)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-11T09:22:31.372025Z",
          "iopub.execute_input": "2024-12-11T09:22:31.372302Z",
          "iopub.status.idle": "2024-12-11T09:22:34.650861Z",
          "shell.execute_reply.started": "2024-12-11T09:22:31.372274Z",
          "shell.execute_reply": "2024-12-11T09:22:34.649959Z"
        },
        "id": "6mjKbU3MJ9jk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"I had a surgery which ended up with some failures. What can I do to fix it?\"\n",
        "response = generate_response_top_k_top_p(question, model, tokenizer)\n",
        "print(\"Question:\", question)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-11T09:30:21.747917Z",
          "iopub.execute_input": "2024-12-11T09:30:21.748826Z",
          "iopub.status.idle": "2024-12-11T09:30:24.334101Z",
          "shell.execute_reply.started": "2024-12-11T09:30:21.748786Z",
          "shell.execute_reply": "2024-12-11T09:30:24.332964Z"
        },
        "id": "9SPlEv1KJ9jk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"I have pain in my back\"\n",
        "response = generate_response_top_k_top_p(question, model, tokenizer)\n",
        "print(\"Question:\", question)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-11T09:31:26.62123Z",
          "iopub.execute_input": "2024-12-11T09:31:26.62159Z",
          "iopub.status.idle": "2024-12-11T09:31:29.215408Z",
          "shell.execute_reply.started": "2024-12-11T09:31:26.621558Z",
          "shell.execute_reply": "2024-12-11T09:31:29.214474Z"
        },
        "id": "BrJRY47gJ9jl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how to check if i have cancer\"\n",
        "response = generate_response_top_k_top_p(question, model, tokenizer)\n",
        "print(\"Question:\", question)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-11T09:31:50.895294Z",
          "iopub.execute_input": "2024-12-11T09:31:50.896148Z",
          "iopub.status.idle": "2024-12-11T09:31:53.483541Z",
          "shell.execute_reply.started": "2024-12-11T09:31:50.896108Z",
          "shell.execute_reply": "2024-12-11T09:31:53.482554Z"
        },
        "id": "NStNNn3sJ9jl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"i think i have liver failure what should i do\"\n",
        "response = generate_response_top_k_top_p(question, model, tokenizer)\n",
        "print(\"Question:\", question)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-11T09:32:36.533973Z",
          "iopub.execute_input": "2024-12-11T09:32:36.534308Z",
          "iopub.status.idle": "2024-12-11T09:32:39.179134Z",
          "shell.execute_reply.started": "2024-12-11T09:32:36.534281Z",
          "shell.execute_reply": "2024-12-11T09:32:39.177688Z"
        },
        "id": "LjDgcnuEJ9jl"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}